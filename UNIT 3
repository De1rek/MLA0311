import numpy as np
import random

grid=np.array([
    [0,1,0,0,-1],
    [0,0,0,1,0],
    [-1,0,0,0,0],
    [0,1,-1,0,0],
    [0,0,0,1,0]
])

actions=[0,1,2,3]
alpha=0.01
gamma=0.9
episodes=3000

theta=np.zeros((5,5,4))

def softmax(x):
    e=np.exp(x-np.max(x))
    return e/e.sum()

def step(state,action):
    r,c=state
    if action==0:
        r=max(0,r-1)
    elif action==1:
        r=min(4,r+1)
    elif action==2:
        c=max(0,c-1)
    else:
        c=min(4,c+1)
    reward=grid[r,c]
    return (r,c),reward

for _ in range(episodes):
    state=(0,0)
    episode=[]
    for _ in range(50):
        probs=softmax(theta[state[0],state[1]])
        action=np.random.choice(actions,p=probs)
        next_state,reward=step(state,action)
        episode.append((state,action,reward))
        state=next_state
    G=0
    for s,a,r in reversed(episode):
        G=r+gamma*G
        probs=softmax(theta[s[0],s[1]])
        for i in range(4):
            if i==a:
                theta[s[0],s[1],i]+=alpha*G*(1-probs[i])
            else:
                theta[s[0],s[1],i]-=alpha*G*probs[i]

policy=np.zeros((5,5),dtype=str)
dirs=["U","D","L","R"]
for i in range(5):
    for j in range(5):
        policy[i,j]=dirs[np.argmax(theta[i,j])]

print("Optimal Policy:")
print(policy)
